{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src langauge is en. Target language is es.\n",
    "#Load pre-trained embeddings. Using fast-text embeddings on wiki.\n",
    "\n",
    "#Parameters\n",
    "max_vocab = 10000\n",
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the src words first\n",
    "src_lang = 'en'\n",
    "src_emd_path = 'data/wiki.en.vec'\n",
    "\n",
    "src_word2id = {}\n",
    "src_embeddings = []\n",
    "\n",
    "with open(src_emd_path) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i==0:\n",
    "            split = line.split()\n",
    "            assert len(split) == 2\n",
    "            assert emb_dim == int(split[1])\n",
    "        else:\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
    "                vect[0] = 0.01\n",
    "            assert word not in src_word2id\n",
    "            assert vect.shape == (emb_dim, )\n",
    "            src_word2id[word] = len(src_word2id)\n",
    "            src_embeddings.append(vect[None,:])\n",
    "        if i > max_vocab:\n",
    "            break\n",
    "            \n",
    "src_id2word = {}\n",
    "src_id2word = {v: k for k,v in src_word2id.items()}\n",
    "src_embeddings = np.concatenate(src_embeddings,0)\n",
    "src_embeddings = torch.from_numpy(src_embeddings).float()\n",
    "src_embeddings = src_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-2.3167e-02 -4.2483e-03 -1.0572e-01  ...   8.9398e-02 -1.5900e-02  1.4866e-01\n",
       "-1.1112e-01 -1.3859e-03 -1.7780e-01  ...   6.3374e-02 -1.2161e-01  3.9339e-02\n",
       "-6.5334e-02 -9.3031e-02 -1.7571e-02  ...   1.6642e-01 -1.3079e-01  3.5397e-02\n",
       "                ...                   ⋱                   ...                \n",
       " 3.4931e-02 -2.5885e-02 -2.5731e-01  ...  -2.5291e-02  1.9552e-01  1.5896e-01\n",
       "-1.6874e-01 -8.9404e-04 -4.0138e-02  ...   2.5356e-01 -2.0092e-02  2.0918e-01\n",
       "-9.0509e-02  3.2207e-01 -6.3597e-01  ...   1.4948e-01  1.6384e-01  3.8038e-01\n",
       "[torch.FloatTensor of size 10001x300]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb = nn.Embedding(len(src_word2id), emb_dim, sparse=True)\n",
    "src_emb.weight.data.copy_(src_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dico = Dictionary(src_id2word, src_word2id, src_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.3075e-01 -8.7659e-02 -1.1427e-01  ...  -4.0476e-02 -1.2293e-02  4.2569e-02\n",
       "-3.6446e-01  9.5962e-02 -1.6188e-01  ...  -1.4986e-01  2.3584e-01  1.8541e-01\n",
       "-5.9110e-02 -8.3343e-02 -9.3019e-02  ...  -5.4064e-02  1.7285e-01  1.6713e-01\n",
       "                ...                   ⋱                   ...                \n",
       "-1.3761e-01 -3.4233e-01 -1.4767e-01  ...   1.1511e-01 -2.3792e-03 -2.4035e-01\n",
       " 2.0302e-01 -2.0129e-01 -1.2699e-01  ...  -3.2187e-01  1.5285e-01  7.9039e-02\n",
       "-3.8139e-01 -7.2348e-01 -6.2569e-02  ...  -6.2603e-01  2.7355e-01  6.8966e-02\n",
       "[torch.FloatTensor of size 10001x300]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up embeddings for target language\n",
    "\n",
    "tgt_lang = 'es'\n",
    "tgt_emd_path = 'data/wiki.es.vec'\n",
    "\n",
    "\n",
    "tgt_word2id = {}\n",
    "tgt_id2word = {}\n",
    "tgt_embeddings = []\n",
    "\n",
    "\n",
    "\n",
    "with open(tgt_emd_path) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i==0:\n",
    "            split = line.split()\n",
    "            assert len(split) == 2\n",
    "            assert emb_dim == int(split[1])\n",
    "        else:\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
    "                vect[0] = 0.01\n",
    "            assert word not in tgt_word2id\n",
    "            assert vect.shape == (emb_dim, )\n",
    "            tgt_word2id[word] = len(tgt_word2id)\n",
    "            tgt_embeddings.append(vect[None,:])\n",
    "        if i > max_vocab:\n",
    "            break\n",
    "\n",
    "tgt_id2word = {v:k for k,v in tgt_word2id.items()}\n",
    "\n",
    "tgt_dico = Dictionary(tgt_id2word, tgt_word2id, tgt_lang)\n",
    "tgt_embeddings = np.concatenate(tgt_embeddings,0)\n",
    "tgt_embeddings = torch.from_numpy(tgt_embeddings).float()\n",
    "tgt_embeddings = tgt_embeddings.cuda()\n",
    "\n",
    "tgt_emb = nn.Embedding(len(tgt_word2id), emb_dim, sparse=True)\n",
    "tgt_emb.weight.data.copy_(tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     0     0  ...      0     0     0\n",
       "    0     1     0  ...      0     0     0\n",
       "    0     0     1  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      1     0     0\n",
       "    0     0     0  ...      0     1     0\n",
       "    0     0     0  ...      0     0     1\n",
       "[torch.FloatTensor of size 300x300]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "mapping.weight.data.copy_(torch.diag(torch.ones(emb_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_layers = 2\n",
    "disc_dim_hidden = 2048\n",
    "disc_dropout = 0\n",
    "disc_inp_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.disc_layers = disc_layers\n",
    "        self.disc_dim_hidden = disc_dim_hidden\n",
    "        self.disc_dropout = disc_dropout\n",
    "        self.disc_inp_dropout = disc_inp_dropout\n",
    "        \n",
    "        layers = [nn.Dropout(self.disc_inp_dropout)]\n",
    "        for i in range(self.disc_layers + 1):\n",
    "            input_dim = self.emb_dim if i == 0 else self.disc_dim_hidden\n",
    "            output_dim = 1 if i==self.disc_layers else self.disc_dim_hidden\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            if i < self.disc_layers:\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "                layers.append(nn.Dropout(self.disc_dropout))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).view(-1)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (layers): Sequential(\n",
       "    (0): Dropout(p=0.1)\n",
       "    (1): Linear(in_features=300, out_features=2048)\n",
       "    (2): LeakyReLU(0.2)\n",
       "    (3): Dropout(p=0)\n",
       "    (4): Linear(in_features=2048, out_features=2048)\n",
       "    (5): LeakyReLU(0.2)\n",
       "    (6): Dropout(p=0)\n",
       "    (7): Linear(in_features=2048, out_features=1)\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb.cuda()\n",
    "tgt_emb.cuda()\n",
    "mapping.cuda()\n",
    "discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings before the training\n",
    "#Add it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_fn = optim.SGD\n",
    "optim_params = {'lr': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_optimizer = optim_fn(mapping.parameters(), **optim_params)\n",
    "disc_optimizer = optim_fn(discriminator.layers.parameters(), **optim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 1000000\n",
    "batch_size = 32\n",
    "disc_steps = 5\n",
    "disc_most_freq = 75000\n",
    "disc_smooth = 0.1\n",
    "map_beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_xy(volatile):\n",
    "    \"\"\"\n",
    "    Get discriminator input batch / output target.\n",
    "    \"\"\"\n",
    "    # select random word IDs\n",
    "    bs = batch_size\n",
    "    mf = disc_most_freq\n",
    "    assert mf <= min(len(src_dico), len(tgt_dico))\n",
    "    src_ids = torch.LongTensor(bs).random_(mf)\n",
    "    tgt_ids = torch.LongTensor(bs).random_(mf)\n",
    "    src_ids = src_ids.cuda()\n",
    "    tgt_ids = tgt_ids.cuda()\n",
    "\n",
    "    # get word embeddings\n",
    "    src_emb = src_emb(Variable(src_ids, volatile=True))\n",
    "    tgt_emb = tgt_emb(Variable(tgt_ids, volatile=True))\n",
    "    src_emb = mapping(Variable(src_emb.data, volatile=volatile))\n",
    "    tgt_emb = Variable(tgt_emb.data, volatile=volatile)\n",
    "\n",
    "    # input / target\n",
    "    x = torch.cat([src_emb, tgt_emb], 0)\n",
    "    y = torch.FloatTensor(2 * bs).zero_()\n",
    "    y[:bs] = 1 - disc_smooth\n",
    "    y[bs:] = dis_smooth\n",
    "    y = Variable(y.cuda())\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to study the underlying math.\n",
    "def orthogonalize():\n",
    "    \"\"\"\n",
    "    Orthogonalize the mapping.\n",
    "    \"\"\"\n",
    "    W = mapping.weight.data\n",
    "    beta = map_beta\n",
    "    W.copy_((1 + beta) * W - beta * W.mm(W.transpose(0, 1).mm(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dis(self, to_log):\n",
    "    \"\"\"\n",
    "    Evaluate discriminator predictions and accuracy.\n",
    "    \"\"\"\n",
    "    bs = 128\n",
    "    src_preds = []\n",
    "    tgt_preds = []\n",
    "\n",
    "    discriminator.eval()\n",
    "\n",
    "    for i in range(0, src_emb.num_embeddings, bs):\n",
    "        emb = Variable(src_emb.weight[i:i + bs].data, volatile=True)\n",
    "        preds = discriminator(mapping(emb))\n",
    "        src_preds.extend(preds.data.cpu().tolist())\n",
    "\n",
    "    for i in range(0,tgt_emb.num_embeddings, bs):\n",
    "        emb = Variable(tgt_emb.weight[i:i + bs].data, volatile=True)\n",
    "        preds = discriminator(emb)\n",
    "        tgt_preds.extend(preds.data.cpu().tolist())\n",
    "\n",
    "    src_pred = np.mean(src_preds)\n",
    "    tgt_pred = np.mean(tgt_preds)\n",
    "    print(\"Discriminator source / target predictions: %.5f / %.5f\"\n",
    "                % (src_pred, tgt_pred))\n",
    "\n",
    "    src_accu = np.mean([x >= 0.5 for x in src_preds])\n",
    "    tgt_accu = np.mean([x < 0.5 for x in tgt_preds])\n",
    "    dis_accu = ((src_accu * src_emb.num_embeddings + tgt_accu * tgt_emb.num_embeddings) /\n",
    "                    (src_emb.num_embeddings + tgt_emb.num_embeddings))\n",
    "    print(\"Discriminator source / target / global accuracy: %.5f / %.5f / %.5f\"\n",
    "                    % (src_accu, tgt_accu, dis_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning loop for Adversarial Training\n",
    "\n",
    "def adv_training(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        n_words_proc = 0\n",
    "        for n_iter in range(0,epoch_size, batch_size):\n",
    "            for _ in range(disc_steps): # Discriminator training\n",
    "                discriminator.train() # sets the module in training mode ex adds dropout and batchnorm\n",
    "                x,y = get_disc_xy(volatile=True)\n",
    "                preds = discriminator(Variable(x.data))\n",
    "                loss = F.binary_cross_entropy(preds,y)\n",
    "                disc_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                # Can add clipping if needed\n",
    "            \n",
    "            #Mapping step\n",
    "            discriminator.eval() # Puts the module in evaluation mode.\n",
    "            x, y = get_disc_xy(volatile=False)\n",
    "            preds = discriminator(Variable(x.data))\n",
    "            loss = F.binary_cross_entropy(preds, 1-y)\n",
    "            map_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            map_optmizer.step()\n",
    "            orthogonalize()\n",
    "            n_words_proc += 2 * batch_size\n",
    "            \n",
    "        # embeddings / discriminator evaluation\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
